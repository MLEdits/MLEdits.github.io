<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                emb_name = emb.name
                if self._world_size &gt; 1:
                    &#47&#47 gather gradients from all other processes
                    for i in <a id="change">range(self._world_size)</a>:
                        if i != self._rank:
                            idx_shmem_name = &quotidx_{}_{}_{}&quot.format(emb_name, i, self._rank)
                            grad_shmem_name = &quotgrad_{}_{}_{}&quot.format(emb_name, i, self._rank)
                            size = self._opt_meta[emb_name][i][self._rank]

                            &#47&#47 Retrive shared memory holding the temporal index and gradient
                            &#47&#47 tensor that is sent to current training process
                            if idx_shmem_name not in self._shared_cache[emb_name] or \
                                self._shared_cache[emb_name][idx_shmem_name].shape[0] &lt; size:
                                <a id="change">idx_shmem</a> = get_shared_mem_array(idx_shmem_name, \
                                    (size * 2 + 2,), idx_dtype)
                                grad_shmem = get_shared_mem_array(grad_shmem_name, \
                                    (size * 2 + 2, grad_dim), grad_dtype)</code></pre><h3>After Change</h3><pre><code class='java'>
                    &#47&#47     3.b Do gradient update
                    &#47&#47   4. Done
                    idx_split = th.remainder(idx, self._world_size).long()
                    for i in <a id="change">range(self._world_size)</a>:
                        mask = idx_split == i
                        idx_i = idx[mask]
                        grad_i = grad[mask]

                        if i == self._rank:
                            shared_emb[emb_name][0].append(idx_i)
                            shared_emb[emb_name][1].append(grad_i)
                        else:
                            &#47&#47 currently nccl does not support Alltoallv operation
                            &#47&#47 we need to use CPU shared memory to share gradient
                            &#47&#47 across processes
                            idx_i = idx_i.to(th.device(&quotcpu&quot))
                            grad_i = grad_i.to(th.device(&quotcpu&quot))
                            idx_shmem_name = &quotidx_{}_{}_{}&quot.format(emb_name, self._rank, i)
                            grad_shmem_name = &quotgrad_{}_{}_{}&quot.format(emb_name, self._rank, i)

                            &#47&#47 Create shared memory to hold temporary index and gradient tensor for
                            &#47&#47 cross-process send and recv.
                            if idx_shmem_name not in self._shared_cache[emb_name] or \
                                self._shared_cache[emb_name][idx_shmem_name].shape[0] \
                                    &lt; idx_i.shape[0]:

                                if idx_shmem_name in self._shared_cache[emb_name]:
                                    self.shmem_buffer_holder.append(
                                        self._shared_cache[emb_name][idx_shmem_name])
                                    self.shmem_buffer_holder.append(
                                        self._shared_cache[emb_name][grad_shmem_name])

                                &#47&#47 The total number of buffers is the number of NodeEmbeddings *
                                &#47&#47 world_size * (world_size - 1). The minimun buffer size is 128.
                                &#47&#47
                                &#47&#47 We extend the buffer by idx_i.shape[0] * 2 to avoid
                                &#47&#47 frequent shared memory allocation.
                                &#47&#47 The overall buffer cost will be smaller than three times
                                &#47&#47 the maximum memory requirement for sharing gradients.
                                buffer_size = 128 if idx_i.shape[0] &lt; 128 else idx_i.shape[0] * 2
                                idx_shmem = create_shared_mem_array(idx_shmem_name, \
                                    (buffer_size,), idx_dtype)
                                grad_shmem = create_shared_mem_array(grad_shmem_name, \
                                    (buffer_size, grad_dim), grad_dtype)
                                self._shared_cache[emb_name][idx_shmem_name] = idx_shmem
                                self._shared_cache[emb_name][grad_shmem_name] = grad_shmem

                            &#47&#47 Fill shared memory with temporal index tensor and gradient tensor
                            self._shared_cache[emb_name][idx_shmem_name][:idx_i.shape[0]] \
                                = idx_i
                            self._shared_cache[emb_name][grad_shmem_name][:idx_i.shape[0]] \
                                = grad_i
                            self._opt_meta[emb_name][self._rank][i] = idx_i.shape[0]
                else:
                    shared_emb[emb_name][0].append(idx)
                    shared_emb[emb_name][1].append(grad)

            &#47&#47 make sure the idx shape is passed to each process through opt_meta
            if self._world_size &gt; 1:
                th.distributed.barrier()
            for emb in self._params: &#47&#47 pylint: disable=too-many-nested-blocks
                emb_name = emb.name
                if self._world_size &gt; 1:
                    &#47&#47 gather gradients from all other processes
                    for i in range(self._world_size):
                        if i != self._rank:
                            idx_shmem_name = &quotidx_{}_{}_{}&quot.format(emb_name, i, self._rank)
                            grad_shmem_name = &quotgrad_{}_{}_{}&quot.format(emb_name, i, self._rank)
                            size = self._opt_meta[emb_name][i][self._rank]

                            &#47&#47 Retrive shared memory holding the temporal index and gradient
                            &#47&#47 tensor that is sent to current training process
                            if idx_shmem_name not in self._shared_cache[emb_name] or \
                                self._shared_cache[emb_name][idx_shmem_name].shape[0] &lt; size:
                                buffer_size = 128 if size &lt; 128 else size * 2
                                <a id="change">idx_shmem</a> = get_shared_mem_array(idx_shmem_name, \
                                    (buffer_size,), idx_dtype)
                                grad_shmem = get_shared_mem_array(grad_shmem_name, \
                                    (buffer_size, grad_dim), grad_dtype)</code></pre>