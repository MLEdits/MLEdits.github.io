<html><h3>9a76bb5272c3f8662b266410c0c5b0dbce3ec025,open_seq2seq/parts/rnns/attention_wrapper.py,LocationSensitiveAttention,__init__,#LocationSensitiveAttention#Any#Any#Any#Any#Any#Any#Any#Any#Any#,716
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    if dtype is None:
      dtype = dtypes.float32
    wrapped_probability_fn = lambda score, _: probability_fn(score)
    <a id="change">super(LocationSensitiveAttention, self).__init__(
        query_layer=layers_core.Dense(
            num_units, name="query_layer", use_bias=False, dtype=dtype
        ),
        memory_layer=layers_core.Dense(
            num_units, name="memory_layer", use_bias=False, dtype=dtype
        ),
        memory=memory,
        probability_fn=wrapped_probability_fn,
        memory_sequence_length=memory_sequence_length,
        score_mask_value=score_mask_value,
        name=name
    )</a>
    <a id="change">self.location_layer</a> = LocationLayer(32, 32, num_units)
    <a id="change">self._num_units</a> = num_units
    <a id="change">self._name</a> = name
    self.use_bias = use_bias
    self._use_state = use_state
    <a id="change">self.cumulative_location = self.initial_state(
        self._batch_size, dtype
    )</a>

  def __call__(self, query, state):
    Score the query based on the keys, values, and location.
</code></pre><h3>After Change</h3><pre><code class='java'>
from tensorflow.python.layers.convolutional import Conv1D
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import check_ops
from tensorflow.py<a id="change">thon</a>.ops import clip_ops
from tensorflow.python.ops import functional_ops
from tensorflow.python.ops import init_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import nn_ops
from tensorflow.python.ops import random_ops
from tensorflow.python.ops import rnn_cell_impl
from tensorflow.python.ops import tensor_array_ops
from tensorflow.python.ops import variable_scope
from tensorflow.python.util import nest

__all__ = [
    "AttentionMechanism", "AttentionWrapper", "AttentionWrapperState",
    "LuongAttention", "BahdanauAttention", "hardmax", "safe_cumprod",
    "monotonic_attention", "BahdanauMonotonicAttention",
    "LuongMonotonicAttention", "LocationSensitiveAttention"
]

_zero_state_tensors = rnn_cell_impl._zero_state_tensors  &#47&#47 pylint: disable=protected-access


class AttentionMechanism(object):

  @property
  def alignments_size(self):
    raise NotImplementedError

  @property
  def state_size(self):
    raise NotImplementedError


def _prepare_memory(memory, memory_sequence_length, check_inner_dims_defined):
  Convert to tensor and possibly mask `memory`.

  Args:
    memory: `Tensor`, shaped `[batch_size, max_time, ...]`.
    memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.
    check_inner_dims_defined: Python boolean.  If `True`, the `memory`
      argument&quots shape is checked to ensure all but the two outermost
      dimensions are fully defined.

  Returns:
    A (possibly masked), checked, new `memory`.

  Raises:
    ValueError: If `check_inner_dims_defined` is `True` and not
      `memory.shape[2:].is_fully_defined()`.
  
  memory = nest.map_structure(
      lambda m: ops.convert_to_tensor(m, name="memory"), memory
  )
  if memory_sequence_length is not None:
    memory_sequence_length = ops.convert_to_tensor(
        memory_sequence_length, name="memory_sequence_length"
    )
  if check_inner_dims_defined:

    def _check_dims(m):
      if not m.get_sha<a id="change">pe()[2:].is_fully_defined()</a>:
        raise ValueError(
 <a id="change">           "Expe</a>cted memory %s to have fully defined inner dims, "
            "but saw shape: %s" % (m.name, m.get_shape())
        )

    nest.map_structure(_check_dims, m<a id="change">emory)
  if memory_sequence_length is None:
    seq_len_mask = None
  else:
  </a>  seq_len_mask = array_ops.sequence_mask(
        memory_sequence_length,
        maxlen=array_ops.shape(nest.flatten(memory)[0])[1],
        dtype=nest.flatten(memory)[0].dtype</code></pre><img src="26237995.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 14</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/9a76bb5272c3f8662b266410c0c5b0dbce3ec025#diff-5ea66fe7f631e4ee8612345112d4e76afe92cc9cbfbe8da68238c99eb762f911L42' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 9a76bb5272c3f8662b266410c0c5b0dbce3ec025</div><div id='time'> Time: 2018-08-20</div><div id='author'> Author: jasoli@nvidia.com</div><div id='file'> File Name: open_seq2seq/parts/rnns/attention_wrapper.py</div><div id='class'> Class Name: LocationSensitiveAttention</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/dmlc/gluon-nlp/commit/a947d66d28baaae1302363556a8a18b04fa6aa40#diff-8c38d8d7e6e2acb0239e4561bbe11789150dc07c3e01274ae1e4fd4348f36225L249' target='_blank'>Link</a></div><div id='project'> Project Name: dmlc/gluon-nlp</div><div id='commit'> Commit Name: a947d66d28baaae1302363556a8a18b04fa6aa40</div><div id='time'> Time: 2018-08-16</div><div id='author'> Author: leonard@lausen.nl</div><div id='file'> File Name: gluonnlp/embedding/evaluation.py</div><div id='class'> Class Name: ThreeCosMul</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/281b8bc9b05e9c959c18ce8c6f79e7c3e20b4495#diff-b69ad581af290659556987c88e3c5f9956216f2372e15441331bcdc508ea326cL95' target='_blank'>Link</a></div><div id='project'> Project Name: AKSHAYUBHAT/DeepVideoAnalytics</div><div id='commit'> Commit Name: 281b8bc9b05e9c959c18ce8c6f79e7c3e20b4495</div><div id='time'> Time: 2017-08-22</div><div id='author'> Author: akshayubhat@gmail.com</div><div id='file'> File Name: dvalib/detector.py</div><div id='class'> Class Name: TFDetector</div><div id='method'> Method Name: __init__</div><BR>