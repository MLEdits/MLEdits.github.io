<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

        emb = self.word_lut(input)
        if self.positional_encoding:
            emb = emb + Variable(self.pe[<a id="change">:emb.size(0), :1, :</a>emb.size(2)]
                                 .expand_as(emb))

        &#47&#47 n.b. you can increase performance if you compute W_ih * x for all
        &#47&#47 iterations in parallel, but that&quots only possible if
        &#47&#47 self.input_feed=False
        outputs = []

        &#47&#47 Setup the different types of attention.
        attns = {"std": []}
        if self._copy:
            attns["copy"] = []
        if self._coverage:
            attns["coverage"] = []

        output = init_feed
        coverage = None

        if self.decoder_layer == "transformer":
            <a id="change">output = self.emb_dropout(emb.transpose(0, 1).contiguous())</a>
            src_context = context.transpose(0, 1).contiguous()
            for i in range(self.layers):
                output, attn = self.transformer[i](output, src_context,
                                                   src[:, :, 0], input)</code></pre><h3>After Change</h3><pre><code class='java'>
        if has_transformer_hidden:
            input = torch.cat([hidden[0].squeeze(2), input], 0)

        emb = self.embeddings(<a id="change">input.unsqueeze(2)</a>)

        &#47&#47 n.b. you can increase performance if you compute W_ih * x for all
        &#47&#47 iterations in parallel, but that&quots only possible if</code></pre>