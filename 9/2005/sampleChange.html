<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        if not self._added_to_vocabulary and hasattr(self.tokenizer, "vocab"):
            self._add_encoding_to_vocabulary(vocabulary)
            self._added_to_vocabulary = True
        <a id="change">token_text = [token.text for token in tokens]</a>
        <a id="change">indices = self.tokenizer.convert_tokens_to_ids(token_text)</a>

        return {index_name: indices}

    @overrides</code></pre><h3>After Change</h3><pre><code class='java'>
            self._added_to_vocabulary = True

        indices: List[int] = []
        <a id="change">for token in tokens:
            if getattr(token, "text_id", None) is not None:
                &#47&#47 `text_id` being set on the token means that we aren&quott using the vocab, we just use
                &#47&#47 this id instead. Id comes from the pretrained vocab.
                &#47&#47 &#47&#47 It computed in PretrainedTransformerTokenizer.
                indices.append(token.text_id)
            else:
                raise KeyError(
                    Using PretrainedTransformerIndexer but field text_id is
                                not set for the following token: {token.text}
                )

       </a> return {index_name: indices}

    @overrides
    def get_padding_lengths(self, token: int) -&gt; Dict[str, int]:</code></pre>