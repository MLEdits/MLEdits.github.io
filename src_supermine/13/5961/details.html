<html><h3>9a76bb5272c3f8662b266410c0c5b0dbce3ec025,open_seq2seq/parts/rnns/attention_wrapper.py,LocationSensitiveAttention,__init__,#LocationSensitiveAttention#Any#Any#Any#Any#Any#Any#Any#Any#Any#,716
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    
    if probability_fn is None:
      probability_fn = nn_ops.softmax
    <a id="change">if dtype is None:
      dtype = dtypes.float32
   </a> wrapped_probability_fn = lambda score, _: probability_fn(score)
    super(LocationSensitiveAttention, self).__init__(
        query_layer=layers_core.Dense(
            num_units, name="query_layer", use_bias=False, dtype=dtype
        ),
        memory_layer=layers_core.Dense(
            num_units, name="memory_layer", use_bias=False, dtype=dtype
        ),
        memory=memory,
        probability_fn=wrapped_probability_fn,
        memory_sequence_length=memory_sequence_length,
        score_mask_value=score_mask_value,
        name=name
    )
    self.location_layer = LocationLayer(32, 32, num_units)
    self._num_units = num_units
    self._name = name
    self.use_bias = use_bias
    self._use_state = use_state
    <a id="change">self.cumulative_location</a> = self.initial_state(
        self._batch_size, dtype
    )
</code></pre><h3>After Change</h3><pre><code class='java'>
    A (possibly masked), checked, new `memory`.

  Raises:
    ValueError: If `check_inner_dims_defined` is `True<a id="change">` and not
      `memory.shape[2:].is_fully_defined()`.
  
  memory = nest.map_structure(
      lambda m: ops.convert_to_ten</a>sor(m, name="memory"), memory
  )
  if memory_sequence_length is not None:
    memory_sequence_length = ops.<a id="change">convert_to_tensor(
        memory_sequence_length, name="memory_sequence_length"
    )
  if chec</a>k_inner_dims_defined:

    def _check_dims(m):
      if not m.get_shape()[2:].is_fully_defined():
        raise ValueError(
            "Expected memory %s to have fully defined inner dims, "
            "but saw shape: %s" % (m.name, m.get_shape())
        )

    nest.m<a id="change">ap_structure(_check_d</a>ims, memory)
  if memory_sequence_length is None:
    seq_len_mask = None
  else:</code></pre><img src="25019517.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NVIDIA/OpenSeq2Seq/commit/9a76bb5272c3f8662b266410c0c5b0dbce3ec025#diff-5ea66fe7f631e4ee8612345112d4e76afe92cc9cbfbe8da68238c99eb762f911L42' target='_blank'>Link</a></div><div id='project'> Project Name: NVIDIA/OpenSeq2Seq</div><div id='commit'> Commit Name: 9a76bb5272c3f8662b266410c0c5b0dbce3ec025</div><div id='time'> Time: 2018-08-20</div><div id='author'> Author: jasoli@nvidia.com</div><div id='file'> File Name: open_seq2seq/parts/rnns/attention_wrapper.py</div><div id='class'> Class Name: LocationSensitiveAttention</div><div id='method'> Method Name: __init__</div><BR><BR><div id='link'><a href='https://github.com/ray-project/ray/commit/6373c706615bc94c0a1e7fc564b3d18c3d342d91#diff-8b49c7fb1705b0aa39a5ea95dbabe0b092c77acc9ccafd4b7fa13f6a5382efd3L223' target='_blank'>Link</a></div><div id='project'> Project Name: ray-project/ray</div><div id='commit'> Commit Name: 6373c706615bc94c0a1e7fc564b3d18c3d342d91</div><div id='time'> Time: 2020-04-30</div><div id='author'> Author: ed.nmi.oakes@gmail.com</div><div id='file'> File Name: python/ray/serve/api.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: create_backend</div><BR><BR><div id='link'><a href='https://github.com/PetrochukM/PyTorch-NLP/commit/bb9335bbc981c0541e37a875d79d0ef419008574#diff-804fd2998f37efd35efbb467901833fbc95b5ed95e202dc76c1246dd33d61843L31' target='_blank'>Link</a></div><div id='project'> Project Name: PetrochukM/PyTorch-NLP</div><div id='commit'> Commit Name: bb9335bbc981c0541e37a875d79d0ef419008574</div><div id='time'> Time: 2018-03-25</div><div id='author'> Author: petrochukm@gmail.com</div><div id='file'> File Name: torchnlp/text_encoders/subword_encoder.py</div><div id='class'> Class Name: SubwordEncoder</div><div id='method'> Method Name: __init__</div><BR>