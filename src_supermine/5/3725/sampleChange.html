<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                 weight_initializer=None, bias_initializer=&quotzeros&quot, prefix=None, params=None):
        super(MultiHeadAttentionCell, self).__init__(prefix=prefix, params=params)
        self._base_cell = base_cell
        <a id="change">self._query_units = query_units</a>
        self._key_units = key_units
        self._value_units = value_units
        self._num_heads = num_heads
        self._use_bias = use_bias
        if self._query_units % self._num_heads != 0:
            raise ValueError(<a id="change">&quotIn MultiHeadAttetion, the query_units should be divided exactly&quot
                             &quot by the number of heads. Received query_units={}, num_heads={}&quot
                             .format(key_units, num_heads)</a>)

        if self._key_units % self._num_heads != 0:
            raise ValueError(&quotIn MultiHeadAttetion, the key_units should be divided exactly&quot</code></pre><h3>After Change</h3><pre><code class='java'>
        self._num_heads = num_heads
        self._use_bias = use_bias
        units = {&quotquery&quot: query_units, &quotkey&quot: key_units, &quotvalue&quot: value_units}
        <a id="change">for name, unit in units.items():
            if unit % self._num_heads != 0:
                raise ValueError(
                    &quotIn MultiHeadAttetion, the {name}_units should be divided exactly&quot
                    &quot by the number of heads. Received {name}_units={unit}, num_heads={n}&quot.format(
                        name=name, unit=unit, n=num_heads))
            setattr(self, &quot_{}_units&quot.format(name), unit)
            with self.name_scope():
                setattr(
                    self, &quotproj_{}&quot.format(name),
                    nn.Dense(units=self._query_units, use_bias=self._use_bias, flatten=False,
                             weight_initializer=weight_initializer,
                             bias_initializer=bias_initializer, prefix=&quot{}_&quot.format(name)))

   </a> def __call__(self, query, key, value=None, mask=None):
        Compute the attention.

        Parameters</code></pre>