<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        maximum_decode_length: The maximum length when decoding.
        The meaning of other parameters are similar to TransformerEncoder
        
        <a id="change">return {
            &quotinitializer&quot: None,
            &quotposition_embedder_hparams&quot: None,
            &quotshare_embed_and_transform&quot: True,
            &quottransform_with_bias&quot: True,
            "num_heads":8,
            "num_blocks":6,
            "maximum_decode_length":256,
            "embedding_dropout":0.1,
            &quotattention_dropout&quot:0.1,
            &quotresidual_dropout&quot:0.1,
            &quotposwise_feedforward&quot: {
		&quotname&quot:&quotffn&quot,
		&quotlayers&quot:[
		    {
			&quottype&quot:&quotDense&quot,
			&quotkwargs&quot: {
			    &quotname&quot:&quotconv1&quot,
			    &quotunits&quot:2048,
			    &quotactivation&quot:&quotrelu&quot,
			    &quotuse_bias&quot:True,
			}
		    },
		    {
			&quottype&quot:&quotDropout&quot,
			&quotkwargs&quot: {
			    &quotrate&quot: 0.1,
			}
		    },
		    {
			&quottype&quot:&quotDense&quot,
			&quotkwargs&quot: {
			    &quotname&quot:&quotconv2&quot,
			    &quotunits&quot:512,
			    &quotuse_bias&quot:True,
			    }
		    }
		],
            },
            &quotdim&quot:512,
            &quotalpha&quot:0,
            "name":"decoder",
        }</a>

    def _prepare_tokens_to_embeds(self, tokens):
         a callable function to transform tokens into embeddings.
        token_emb = tf.nn.embedding_lookup(self._embedding, tokens)</code></pre><h3>After Change</h3><pre><code class='java'>
        "name" : str
            Name of the module.
        
        <a id="change">return {
            "num_heads": 8,
            "num_blocks": 6,
            "alpha": 0,
            "initializer": None,
            "position_embedder_hparams": None,
            "embedding_tie": True,
            "output_layer_bias": True,
            "max_decoding_length": 1e10,
            "embedding_dropout": 0.1,
            "attention_dropout": 0.1,
            "residual_dropout": 0.1,
            "poswise_feedforward": default_transformer_poswise_net_hparams(),
            "dim": 512,
            "name": "transformer_decoder",
        }</a>

    def _prepare_tokens_to_embeds(self, tokens):
         a callable function to transform tokens into embeddings.
        token_emb = tf.nn.embedding_lookup(self._embedding, tokens)</code></pre>