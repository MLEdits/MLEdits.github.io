<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
    def __iter__(self):
        paths = cycle(self._paths) if self.is_train else self._paths
        for path in paths:
            cur_dataset = <a id="change">torch.load(path)</a>
            logger.info(&quotLoading dataset from %s, number of examples: %d&quot %
                        (path, len(cur_dataset)))
            cur_dataset.fields = self.fields
            <a id="change">cur_iter = OrderedIterator(
                dataset=cur_dataset,
                batch_size=self.batch_size,
                batch_size_fn=self.batch_size_fn,
                device=self.device,
                train=self.is_train,
                sort=False,
                sort_within_batch=True,
                repeat=False
            )</a>
            for batch in cur_iter:
                yield batch

            cur_dataset.examples = None</code></pre><h3>After Change</h3><pre><code class='java'>
                for batch in self._iter_dataset(path):
                    yield batch
                    num_batches += 1
                    <a id="change">if num_batches % self.num_batches_multiple == 0:
                        return


</a>def max_tok_len(new, count, sofar):
    
    In token batching scheme, the number of sequences is limited
    such that the total number of src/tgt tokens (including padding)</code></pre>