<html><h3>371d682bacdccb770c2ca6e29a44430f4b3ec2d7,auto_ml/predictor.py,Predictor,fit_feature_learning_and_transformation_pipeline,#Predictor#Any#Any#Any#,409
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 Simplified approach: we do not fit the transformation_pipeline on the fl_data. This reduces computation time somewhat, and everything that&quots in fl_data should be a subset of what&quots in our X_df data
        &#47&#47 We can change this in v2, but this is a conscious design decision for now
        &#47&#47 FUTURE IMPROVEMENT: just grab our already_transformed fl_data and X_df
        <a id="change">fl_data = self.transformation_pipeline.transform(fl_data)</a>


        &#47&#47 fit a train_final_estimator
        feature_learning_step = self.train_ml_estimator(fl_estimator_names, self._scorer, fl_data, fl_y, feature_learning=True)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 For performance reasons, I believe it is critical to only have one transformation pipeline, no matter how many estimators we eventually build on top. Getting predictions from a trained estimator is typically super quick. We can easily get predictions from 10 trained models in a production-ready amount of time.But the transformation pipeline is not so quick that we can duplicate it 10 times.
        combined_transformed_data = self.fit_transformation_pipeline(combined_training_data, combined_y, fl_estimator_names[0])

        fl_indices = [i for i in range(len_X_df, <a id="change">combined_transformed_data.shape[0]</a>)]
        fl_data_transformed = combined_transformed_data[fl_indices]

        &#47&#47 fit a train_final_estimator</code></pre><img src="3032940.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/ClimbsRocks/auto_ml/commit/371d682bacdccb770c2ca6e29a44430f4b3ec2d7#diff-893454c3b7604c9e39cf39dfbdd6f1f0b1c6707e4fec48a68ceec16035875121L409' target='_blank'>Link</a></div><div id='project'> Project Name: ClimbsRocks/auto_ml</div><div id='commit'> Commit Name: 371d682bacdccb770c2ca6e29a44430f4b3ec2d7</div><div id='time'> Time: 2017-04-20</div><div id='author'> Author: ClimbsBytes@gmail.com</div><div id='file'> File Name: auto_ml/predictor.py</div><div id='class'> Class Name: Predictor</div><div id='method'> Method Name: fit_feature_learning_and_transformation_pipeline</div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/03e8525394683bb5da7668330cd910c87c7d4501#diff-8f41b7c6f8310ccf5ec23acf0ede4f3b90c12388db593458278544ffb09ae3d5L78' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: 03e8525394683bb5da7668330cd910c87c7d4501</div><div id='time'> Time: 2017-04-03</div><div id='author'> Author: l.fidon@ucl.ac.uk</div><div id='file'> File Name: data_augmentation.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: rand_intensity_normalisation</div><BR><BR><div id='link'><a href='https://github.com/keras-team/autokeras/commit/71432d1890209628e189edf04d7d623160943718#diff-d519c75330811b4f3678de6df72b346f37048b42e92ef5b0482655aa59c00938L182' target='_blank'>Link</a></div><div id='project'> Project Name: keras-team/autokeras</div><div id='commit'> Commit Name: 71432d1890209628e189edf04d7d623160943718</div><div id='time'> Time: 2019-12-18</div><div id='author'> Author: 33369174+Davidsirui@users.noreply.github.com</div><div id='file'> File Name: autokeras/hypermodel/preprocessor.py</div><div id='class'> Class Name: TextToNgramVector</div><div id='method'> Method Name: transform</div><BR>